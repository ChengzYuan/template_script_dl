{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d14c8c3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Write by Chengzhi Yuan @April 29 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2492317",
   "metadata": {},
   "source": [
    "### Some advanced models: \n",
    "\n",
    "###### 一些闲着没事儿或者不想学习了写的模型和简单的特征工程，可能有些效果不好，当个模板用\n",
    "\n",
    "including:\n",
    "\n",
    "- Sparse feature split via chi2select,\n",
    "\n",
    "- XGB, Cascade SVC, Stacking XGB SVC MLP,\n",
    "\n",
    "- Wide and Deep with transpose transformers and MoE,\n",
    "\n",
    "- MoE with transpose transformer expert,\n",
    "\n",
    "- and ablation study, fm, etc.\n",
    "\n",
    "- further ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb54234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import importlib\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "# pdb.set_trace()\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e3ea9",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4d0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features_name = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Name']\n",
    "dense_features_name  = list(set(train_data.columns.tolist()[:-1]) - set(['PassengerId']) - set(sparse_features_name))\n",
    "label_data = train_data['Transported']\n",
    "\n",
    "labels = []\n",
    "for _ in range(label_data.shape[0]):\n",
    "    if label_data[_] == True:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "all_features_name = sparse_features_name + dense_features_name\n",
    "# label_encoder = preprocessing.LabelEncoder()\n",
    "# all_features_list = []\n",
    "# for feature_name in all_features_name:\n",
    "#     train_data[feature_name] = train_data[feature_name].fillna(method = 'pad')\n",
    "#     all_features_list.append(label_encoder.fit_transform(train_data[feature_name]))\n",
    "# all_features = np.array(all_features_list).T\n",
    "\n",
    "\n",
    "# all_features = np.zeros((all_features.shape[0], len(all_features_name)))\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# for idx, feature_name in enumerate(dense_features_name):\n",
    "#     tmp = train_data[feature_name].fillna(train_data[feature_name].mean())\n",
    "#     dense_features[:, idx] = scaler.fit_transform(np.array(tmp).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "sparse_features_list = []\n",
    "for feature_name in sparse_features_name:\n",
    "    train_data[feature_name] = train_data[feature_name].fillna(method = 'pad')\n",
    "    sparse_features_list.append(label_encoder.fit_transform(train_data[feature_name]))\n",
    "sparse_features = np.array(sparse_features_list).T\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "\n",
    "dense_features = np.zeros((sparse_features.shape[0], len(dense_features_name)))\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "for idx, feature_name in enumerate(dense_features_name):\n",
    "    tmp = train_data[feature_name].fillna(train_data[feature_name].mean())\n",
    "    # dense_features[:, idx] = scaler.fit_transform(np.array(tmp).reshape(-1, 1)).reshape(-1)\n",
    "    dense_features[:, idx] = np.array(tmp)    \n",
    "    # pdb.set_trace()\n",
    "\n",
    "def analysis():\n",
    "    tmp = train_data[dense_features_name].values.astype(np.float32)\n",
    "    nanlocations = np.argwhere(np.isnan(tmp))\n",
    "    nanattributesindex = nanlocations[:, -1].tolist()\n",
    "\n",
    "    ret = [nanattributesindex.count(_) for _ in range(len(dense_features_name))]\n",
    "    pdb.set_trace()\n",
    "\n",
    "# analysis()\n",
    "# pdb.set_trace()\n",
    "# dense_features = np.array(dense_features_list)\n",
    "\n",
    "'''\n",
    "# chi2 evaluation\n",
    "chi2_model = SelectKBest(chi2, k = len(train_data.columns.tolist()[:-1]) - 1)\n",
    "print ('=================chi2==================')\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "chi2_model.fit_transform(np.concatenate((sparse_features, dense_features), axis = -1), labels)\n",
    "print (chi2_model.scores_)\n",
    "print (chi2_model.pvalues_)\n",
    "print (sparse_features_name + dense_features_name)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "################## tabular data augmentation ####################\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c56c4",
   "metadata": {},
   "source": [
    "### FIXME: test data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f085ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features_testlist = []\n",
    "# label_encoder.fit(sparse_features_name)\n",
    "for feature_name in sparse_features_name:\n",
    "    test_data[feature_name] = test_data[feature_name].fillna(method = 'pad')\n",
    "    sparse_features_testlist.append(label_encoder.fit_transform(test_data[feature_name]))\n",
    "    test_sparse_features = np.array(sparse_features_testlist).T\n",
    "\n",
    "test_dense_features = np.zeros((test_sparse_features.shape[0], len(dense_features_name)))\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "for idx, feature_name in enumerate(dense_features_name):\n",
    "    tmp = test_data[feature_name].fillna(test_data[feature_name].mean())\n",
    "    test_dense_features[:, idx] = scaler.fit_transform(np.array(tmp).reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a1077",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a42a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=================split based \"CyroSleep\"==================\n"
     ]
    }
   ],
   "source": [
    "# all_features = np.concatenate((sparse_features, dense_features), axis = -1)\n",
    "# all_features = sparse_features\n",
    "all_dense_features = dense_features\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "assert all_dense_features.shape[0] == labels.shape[0]\n",
    "\n",
    "# use_fm = 0\n",
    "\n",
    "# CryoSleep classifier\n",
    "print ('\\n')\n",
    "print ('=================split based \"CyroSleep\"==================')\n",
    "\n",
    "all_input_sparsefeatures = np.concatenate((sparse_features[:, 0].reshape(-1, 1), sparse_features[:, 2:].reshape(-1, 4)), axis = -1)\n",
    "# all_input_sparsefeatures = np.concatenate((sparse_features[:, 0].reshape(-1, 1), sparse_features[:, 2:].reshape(-1, 4)), axis = -1)\n",
    "# all_input_sparsefeatures = sparse_features.reshape(-1, 6)\n",
    "\n",
    "\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "\n",
    "### Ablation Study: FM Feature Cross\n",
    "use_fm = 0\n",
    "if use_fm:\n",
    "    fm = lambda tmp: 0.5 * (np.power(np.sum(tmp, axis = 1), 2) - np.sum(np.power(tmp, 2), axis = 1))\n",
    "    fm_feature = fm(np.expand_dims(all_input_sparsefeatures, axis = -1))\n",
    "    assert fm_feature.shape[0] == all_input_sparsefeatures.shape[0]\n",
    "\n",
    "    all_dense_features = np.concatenate((all_dense_features, fm_feature), axis = -1)\n",
    "    assert all_dense_features.shape[-1] == dense_features.shape[-1] + 1\n",
    "\n",
    "assert all_dense_features.shape[0] == labels.shape[0]\n",
    "\n",
    "all_densefeatures_model1 = all_dense_features[train_data[train_data['CryoSleep'] == True].index.tolist()]\n",
    "all_sparsefeatures_model1 = all_input_sparsefeatures[train_data[train_data['CryoSleep'] == True].index.tolist()]\n",
    "all_label_model1    = labels[train_data[train_data['CryoSleep'] == True].index.tolist()]\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "all_densefeatures_model2 = all_dense_features[train_data[train_data['CryoSleep'] == False].index.tolist()]\n",
    "all_sparsefeatures_model2 = all_input_sparsefeatures[train_data[train_data['CryoSleep'] == False].index.tolist()]\n",
    "all_label_model2    = labels[train_data[train_data['CryoSleep'] == False].index.tolist()]\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "# assert (all_features_model1.shape[0] + all_features_model2.shape[0] == train_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a67400",
   "metadata": {},
   "source": [
    "### Naive XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb41493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=================train xgb model=====================\n",
      "[14:29:12] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"colsample_btree\", \"slient\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "model accuarte rate: 0.779\n",
      "[14:29:12] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"colsample_btree\", \"slient\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "accuracy 1: 0.797\n",
      "[14:29:12] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"colsample_btree\", \"slient\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "accuracy 2: 0.789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('\\n')\n",
    "print ('=================train xgb model=====================')\n",
    "xgb = XGBClassifier(learning_rate = 0.01, n_estimators = 20, max_depth = 8, \\\n",
    "                     min_child_weight = 1, gamma = 0., subsample = 1, \\\n",
    "                     colsample_btree = 1, scale_pos_weight = 1, random_state = 100, slient = 0)\n",
    "\n",
    "xgb.fit(all_dense_features[:-1000], labels[:-1000])\n",
    "\n",
    "## FIXME: predict\n",
    "y_pred = xgb.predict(all_dense_features[-1000:, :])\n",
    "y_eval = labels[-1000:]\n",
    "\n",
    "acc_count = 0\n",
    "for _ in range(y_pred.shape[0]):\n",
    "    if y_pred[_] == y_eval[_]: acc_count += 1\n",
    "print ('model accuarte rate: ' + str(acc_count / y_eval.shape[0]))\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "xgb1 = XGBClassifier(learning_rate = 0.01, n_estimators = 20, max_depth = 8, \\\n",
    "                     min_child_weight = 1, gamma = 0., subsample = 1, \\\n",
    "                     colsample_btree = 1, scale_pos_weight = 1, random_state = 100, slient = 0)\n",
    "\n",
    "xgb1.fit(np.concatenate((all_densefeatures_model1[:-1000], all_sparsefeatures_model1[:-1000]), axis = -1), \\\n",
    "         all_label_model1[:-1000])\n",
    "\n",
    "pred_xgb1 = xgb1.predict(np.concatenate((all_densefeatures_model1[-1000:], all_sparsefeatures_model1[-1000:]), axis = -1))\n",
    "cal_acc = lambda pred, labels: np.sum(pred == labels) / pred.shape[0]\n",
    "print ('accuracy 1: ' + str(cal_acc(pred_xgb1, all_label_model1[-1000:])))\n",
    "\n",
    "\n",
    "\n",
    "xgb2 = XGBClassifier(learning_rate = 0.01, n_estimators = 20, max_depth = 8, \\\n",
    "                     min_child_weight = 1, gamma = 0., subsample = 1, \\\n",
    "                     colsample_btree = 1, scale_pos_weight = 1, random_state = 100, slient = 0)\n",
    "\n",
    "\n",
    "xgb2.fit(np.concatenate((all_densefeatures_model2[:-1000], all_sparsefeatures_model2[:-1000]), axis = -1), \\\n",
    "         all_label_model2[:-1000])\n",
    "pred_xgb2 = xgb2.predict(np.concatenate((all_densefeatures_model2[-1000:], all_sparsefeatures_model2[-1000:]), axis = -1))\n",
    "print ('accuracy 2: ' + str(cal_acc(pred_xgb2, all_label_model2[-1000:])))\n",
    "\n",
    "\n",
    "\n",
    "# pdb.set_trace()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303629a",
   "metadata": {},
   "source": [
    "### Cascade SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da4e3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy: 0.821\n",
      "model2 accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "base_svc = SVC()\n",
    "\n",
    "chains1 = [ClassifierChain(base_svc, order = \"random\", random_state = i) for i in range(10)]\n",
    "for chain1 in chains1:\n",
    "    # pdb.set_trace()\n",
    "    chain1.fit(np.concatenate((all_densefeatures_model1[:-1000], all_sparsefeatures_model1[:-1000]), axis = -1),\\\n",
    "              all_label_model1[:-1000].reshape(-1, 1))\n",
    "\n",
    "model1_pred_chains = np.array([chain1.predict(np.concatenate((all_densefeatures_model1[-1000:], all_sparsefeatures_model1[-1000:]), axis = -1)) \\\n",
    "                               for chain1 in chains1])\n",
    "\n",
    "def get_ret(tmp: np.array) -> np.array:\n",
    "    ret = np.zeros((tmp.shape[0], 1))\n",
    "    for _ in range(tmp.shape[0]):\n",
    "        ret[_, :] = np.argmax(np.bincount(tmp[_, :].reshape(-1).astype(np.int64)))\n",
    "    # pdb.set_trace()\n",
    "    return ret\n",
    "\n",
    "pred_cascadesvc1 = get_ret(model1_pred_chains.transpose(1, 0, 2)).reshape(-1)\n",
    "cal_acc = lambda pred, labels: np.sum(pred == labels) / pred.shape[0]\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "print ('model1 accuracy: ' + str(cal_acc(pred_cascadesvc1, all_label_model1[-1000:])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chains2 = [ClassifierChain(base_svc, order = \"random\", random_state = i) for i in range(10)]\n",
    "for chain2 in chains2:\n",
    "    # pdb.set_trace()\n",
    "    chain2.fit(np.concatenate((all_densefeatures_model2[:-1000], all_sparsefeatures_model2[:-1000]), axis = -1),\\\n",
    "              all_label_model2[:-1000].reshape(-1, 1))\n",
    "\n",
    "model2_pred_chains = np.array([chain2.predict(np.concatenate((all_densefeatures_model2[-1000:], all_sparsefeatures_model2[-1000:]), axis = -1)) \\\n",
    "                               for chain2 in chains2])\n",
    "\n",
    "pred_cascadesvc2 = get_ret(model2_pred_chains.transpose(1, 0, 2)).reshape(-1)\n",
    "\n",
    "print ('model2 accuracy: ' + str(cal_acc(pred_cascadesvc2, all_label_model2[-1000:])))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5f85c",
   "metadata": {},
   "source": [
    "### Wide&Deep (with transpose transformer and MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f61f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------model1----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▌                                                                                      | 1/5 [00:23<01:35, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 0 | loss: 0.6826862096786499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████▏                                                                | 2/5 [00:47<01:10, 23.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 1 | loss: 0.5035728812217712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████████████████████▊                                           | 3/5 [01:10<00:47, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 2 | loss: 0.4819357991218567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 4/5 [01:34<00:23, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 3 | loss: 0.43643808364868164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:57<00:00, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 4 | loss: 0.4824811816215515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82\n",
      "----------------model2----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▌                                                                                      | 1/5 [00:50<03:23, 50.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 0 | loss: 0.7101141810417175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████▏                                                                | 2/5 [01:42<02:33, 51.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 1 | loss: 0.5491712689399719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████████████████████▊                                           | 3/5 [02:33<01:42, 51.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 2 | loss: 0.8188786506652832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 4/5 [03:24<00:51, 51.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 3 | loss: 0.8443120718002319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [04:14<00:00, 50.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 4 | loss: 0.5075828433036804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.712\n",
      "----------------model3----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████▌                                                                                      | 1/5 [01:14<04:56, 74.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 0 | loss: 0.7096976041793823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████████████████████████▏                                                                | 2/5 [02:29<03:43, 74.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 1 | loss: 0.8484169840812683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████████████████████▊                                           | 3/5 [03:43<02:29, 74.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 2 | loss: 0.8182586431503296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████▍                     | 4/5 [04:58<01:14, 74.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 3 | loss: 0.7906532287597656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [06:13<00:00, 74.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 4 | loss: 0.7560297250747681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.564\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import math\n",
    "import mixture_of_experts as moe\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "train_set_1 = Data.TensorDataset(torch.FloatTensor(all_densefeatures_model1[:-1000]), \\\n",
    "                                 torch.FloatTensor(all_sparsefeatures_model1[:-1000]), \\\n",
    "                                 torch.FloatTensor(all_label_model1[:-1000].reshape(-1, 1)))\n",
    "eval_set_1 = Data.TensorDataset(torch.FloatTensor(all_densefeatures_model1[-1000:]), \\\n",
    "                                torch.FloatTensor(all_sparsefeatures_model1[-1000:]), \\\n",
    "                                torch.FloatTensor(all_label_model1[-1000:].reshape(-1, 1)))\n",
    "\n",
    "train_set_2 = Data.TensorDataset(torch.FloatTensor(all_densefeatures_model2[:-1000]), \\\n",
    "                                 torch.FloatTensor(all_sparsefeatures_model2[:-1000]), \\\n",
    "                                 torch.FloatTensor(all_label_model2[:-1000]).reshape(-1, 1))\n",
    "eval_set_2 = Data.TensorDataset(torch.FloatTensor(all_densefeatures_model2[-1000:]), \\\n",
    "                                torch.FloatTensor(all_sparsefeatures_model2[-1000:]), \\\n",
    "                                torch.FloatTensor(all_label_model2[-1000:]).reshape(-1, 1))\n",
    "\n",
    "\n",
    "train_set_3 = Data.TensorDataset(torch.FloatTensor(all_dense_features[:-2000]), \\\n",
    "                                 torch.FloatTensor(all_input_sparsefeatures[:-2000]), \\\n",
    "                                 torch.LongTensor(labels[:-2000]).reshape(-1, 1))\n",
    "eval_set_3 = Data.TensorDataset(torch.FloatTensor(all_dense_features[-2000:]), \\\n",
    "                                torch.FloatTensor(all_input_sparsefeatures[-2000:]), \\\n",
    "                                torch.LongTensor(labels[-2000:]).reshape(-1, 1))\n",
    "# pdb.set_trace()\n",
    "\n",
    "train_loader_1 = Data.DataLoader(train_set_1, batch_size = 32, shuffle = True)\n",
    "train_loader_2 = Data.DataLoader(train_set_2, batch_size = 32, shuffle = True)\n",
    "train_loader_3 = Data.DataLoader(train_set_3, batch_size = 32, shuffle = True)\n",
    "\n",
    "eval_loader_1 = Data.DataLoader(eval_set_1, batch_size = 32, shuffle = False)\n",
    "eval_loader_2 = Data.DataLoader(eval_set_2, batch_size = 32, shuffle = False)\n",
    "eval_loader_3 = Data.DataLoader(eval_set_3, batch_size = 32, shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.5, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "######################################################################################    \n",
    "'''\n",
    "\n",
    "Transposed transformers with the naive MoE\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb_dense = nn.Linear(1, 256)\n",
    "        self.emb_sparse = nn.Sequential(nn.Linear(5, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
    "                                        nn.Linear(256, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "                                        nn.Linear(64, 32))\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model = 256, dropout = 0.5)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model = 256, nhead = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers = 3)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers = 3)\n",
    "        \n",
    "        self.moe_model = moe.MoE(dim = 256, num_experts = 8, hidden_dim = 256 * 4, activation = nn.ReLU, \n",
    "                                 second_policy_train = 'random', second_policy_eval = 'random',\n",
    "                                 second_threshold_train = 0.2, second_threshold_eval = 0.2,\n",
    "                                 capacity_factor_train = 1.25, capacity_factor_eval = 2., loss_coef = 1e-2)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(nn.Linear(6 * 256, 32), nn.BatchNorm1d(32), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, sparse_features: torch.FloatTensor) -> torch.FloatTensor:  # \n",
    "        '''\n",
    "        x: dense_feature: [batch_size, 6]\n",
    "        sparse_features: [batch_size, 5]\n",
    "        '''\n",
    "        batch_size, feature_num = x.size()\n",
    "        # pdb.set_trace()\n",
    "        x = self.pos_encoder(self.emb_dense(x.unsqueeze(dim = -1)))\n",
    "        out1 = self.transformer_encoder(x) # out1 shape [batch, feature_num, 256]\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        '''\n",
    "        MoE model\n",
    "        pass\n",
    "        import mixture_of_experts as moe\n",
    "        '''\n",
    "        \n",
    "        out_intermedia, aux_loss = self.moe_model(out1)  # out ret shape == input shape\n",
    "        \n",
    "        out2 = self.transformer_encoder2(out_intermedia.permute(1, 0, 2)).permute(1, 0, 2) \n",
    "        \n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        out3 = F.glu(self.mlp_head(out2.reshape(batch_size, feature_num * 256)))\n",
    "        \n",
    "        sparse_embedding = F.glu(self.emb_sparse(sparse_features))\n",
    "        \n",
    "        final_out = self.fc(out3 + sparse_embedding)\n",
    "        # pdb.set_trace()\n",
    "        return final_out, aux_loss\n",
    "\n",
    "    \n",
    "class train_eval_session():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @staticmethod\n",
    "    def run(model: nn.Module, train_loader_x: Data.DataLoader, eval_loader_x: Data.DataLoader):\n",
    "        # torch.compile(model) # for torch 2.0 and above\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        model.train()\n",
    "        n_epochs = 5\n",
    "        for current_epoch in trange(n_epochs):\n",
    "            for _, batch_data in enumerate(train_loader_x):\n",
    "                train_features, train_sparse_features, labels = batch_data[0].to(device), batch_data[1].to(device), batch_data[2].to(device)\n",
    "            \n",
    "                # pdb.set_trace()\n",
    "            \n",
    "                pred, aux_loss = model(train_features, train_sparse_features)\n",
    "                # pdb.set_trace()\n",
    "                loss = criterion(pred, labels.type(torch.float)) + aux_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print (loss.cpu().item())\n",
    "            print ('current epoch: ' + str(current_epoch) + ' | ' + 'loss: ' + str(loss.cpu().item()))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "            model.eval()\n",
    "        \n",
    "            all_eval_pred = []\n",
    "            all_eval_labels = []\n",
    "            for _, eval_data in enumerate(eval_loader_x):\n",
    "                eval_features, eval_sparse_features, eval_labels = eval_data[0].to(device), eval_data[1].to(device), eval_data[2].to(device)\n",
    "            \n",
    "                pred_out, _ = model(eval_features, eval_sparse_features) # eval features shape [batch_size, feature_num]  \n",
    "                # pdb.set_trace()\n",
    "\n",
    "                all_eval_pred  += (pred_out > 0.5).long().view(-1).tolist()   # for sigmoid\n",
    "                # all_eval_pred   += torch.max(pred_out, dim = -1)[1].view(-1).tolist()  # for softmax\n",
    "                all_eval_labels += eval_labels.view(-1).tolist()\n",
    "        \n",
    "            # pdb.set_trace()\n",
    "        \n",
    "            acc = np.sum(np.array(all_eval_pred) == np.array(all_eval_labels)) / np.array(all_eval_pred).shape[0]\n",
    "            print ('accuracy: ' + str(acc))        \n",
    "\n",
    "\n",
    "model1 = model(); model2 = model(); model3 = model()\n",
    "\n",
    "print ('----------------model1----------------')\n",
    "train_eval_session.run(model1, train_loader_1, eval_loader_1)\n",
    "\n",
    "print ('----------------model2----------------')\n",
    "train_eval_session.run(model2, train_loader_2, eval_loader_2)\n",
    "\n",
    "print ('----------------model3----------------')\n",
    "train_eval_session.run(model3, train_loader_3, eval_loader_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################    \n",
    "'''\n",
    "\n",
    "MoE with the transposed transformer\n",
    "\n",
    "'''\n",
    "class tranpose_transformer_expert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model = 256, nhead = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers = 3)\n",
    "        self.transformer_encoder2 = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers = 3)\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        '''\n",
    "        x: (num_of_experts, batch_size, dim)\n",
    "        '''\n",
    "        out1 = self.transformer_encoder(x)\n",
    "        out2 = self.transformer_encoder2(out1.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        return out2\n",
    "\n",
    "    \n",
    "class moe_tt_model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense_emb = nn.Linear(1, 256)\n",
    "        self.sparse_emb_1 = nn.Sequential(nn.Linear(5, 256), nn.ReLU(),\n",
    "                                       nn.Linear(256, 6))\n",
    "        self.sparse_emb_2 = nn.Linear(1, 256)\n",
    "        self.MoE_transpose_transformer = moe.MoE(dim = 256, num_experts = 8, hidden_dim = 256 * 4, activation = nn.ReLU, \n",
    "                                     second_policy_train = 'random', second_policy_eval = 'random',\n",
    "                                     second_threshold_train = 0.2, second_threshold_eval = 0.2,\n",
    "                                     capacity_factor_train = 1.25, capacity_factor_eval = 2., loss_coef = 1e-2)\n",
    "        self.head = nn.Sequential(nn.Linear(256 * 6, 1), nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, sparse_features: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        '''\n",
    "        x: shape (batch_size, dense_feature_num)\n",
    "        sparse_features: shape (batch_size, sparse_feature_num)\n",
    "        ''' \n",
    "        dense_embedding = self.dense_emb(x.unsqueeze(dim = -1)) # [batch_size, 6, 256]\n",
    "        # pdb.set_trace()\n",
    "        sparse_embedding = self.sparse_emb_2(self.sparse_emb_1(sparse_features).unsqueeze(dim = -1)) # [batch_size, 6, 256]\n",
    "        out1, aux_loss = self.MoE_transpose_transformer(dense_embedding + sparse_embedding) # same as input\n",
    "        return self.head(out1.reshape(x.shape[0], -1)), aux_loss\n",
    "\n",
    "    \n",
    "\n",
    "moe_tt_model1 = moe_tt_model()\n",
    "moe_tt_model2 = moe_tt_model()\n",
    "moe_tt_model3 = moe_tt_model()\n",
    "\n",
    "\n",
    "print ('----------------model1----------------')\n",
    "train_eval_session.run(moe_tt_model1, train_loader_1, eval_loader_1)\n",
    "\n",
    "print ('----------------model2----------------')\n",
    "train_eval_session.run(moe_tt_model2, train_loader_2, eval_loader_2)\n",
    "\n",
    "print ('----------------model3----------------')\n",
    "train_eval_session.run(moe_tt_model3, train_loader_3, eval_loader_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2a953",
   "metadata": {},
   "source": [
    "### Stacking Model (xgb, mlp, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb.set_trace()\n",
    "\n",
    "############# model: mlp + xgb + svc stacking method ##################\n",
    "print ('\\n')\n",
    "print ('=================train ensemble model1=====================')\n",
    "xgb1 = XGBClassifier(learning_rate = 0.01, n_estimators = 20, max_depth = 8, \\\n",
    "                     min_child_weight = 1, gamma = 0., subsample = 1, \\\n",
    "                     colsample_btree = 1, scale_pos_weight = 1, random_state = 100, slient = 0)\n",
    "mlp1 = MLPClassifier()\n",
    "svc1 = SVC()\n",
    "\n",
    "xgb1.fit(np.concatenate((all_densefeatures_model1[:-1000], all_sparsefeatures_model1[:-1000]), axis = -1), \\\n",
    "         all_label_model1[:-1000])\n",
    "mlp1.fit(np.concatenate((all_densefeatures_model1[:-1000], all_sparsefeatures_model1[:-1000]), axis = -1), \\\n",
    "         all_label_model1[:-1000])\n",
    "svc1.fit(np.concatenate((all_densefeatures_model1[:-1000], all_sparsefeatures_model1[:-1000]), axis = -1), \\\n",
    "         all_label_model1[:-1000])\n",
    "\n",
    "stacking_model_1 = StackingClassifier(\n",
    "    estimators = [('xgb', xgb1), ('mlp', mlp1), ('svc', svc1)],\n",
    "    final_estimator = LogisticRegression())\n",
    "\n",
    "stacking_model_1.fit(np.concatenate((all_densefeatures_model1[:-1000], all_sparsefeatures_model1[:-1000]), axis = -1), \\\n",
    "                     all_label_model1[:-1000])\n",
    "\n",
    "\n",
    "## FIXME: predict\n",
    "y_pred = stacking_model_1.predict(np.concatenate((all_densefeatures_model1[-1000:], all_sparsefeatures_model1[-1000:]), axis = -1))\n",
    "y_eval = all_label_model1[-1000:]\n",
    "\n",
    "acc_count = 0\n",
    "for _ in range(y_pred.shape[0]):\n",
    "    if y_pred[_] == y_eval[_]: acc_count += 1\n",
    "\n",
    "# pdb.set_trace()\n",
    "        \n",
    "print ('model1 accuarte rate: ' + str(acc_count / y_eval.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "print ('\\n')\n",
    "print ('=================train ensemble model2=====================')\n",
    "xgb2 = XGBClassifier(learning_rate = 0.01, n_estimators = 20, max_depth = 8, \\\n",
    "                     min_child_weight = 1, gamma = 0., subsample = 1, \\\n",
    "                     colsample_btree = 1, scale_pos_weight = 1, random_state = 100, slient = 0)\n",
    "mlp2 = MLPClassifier()\n",
    "svc2 = SVC()\n",
    "\n",
    "xgb2.fit(np.concatenate((all_densefeatures_model2[:-1000], all_sparsefeatures_model2[:-1000]), axis = -1), \\\n",
    "         all_label_model2[:-1000])\n",
    "mlp2.fit(np.concatenate((all_densefeatures_model2[:-1000], all_sparsefeatures_model2[:-1000]), axis = -1), \\\n",
    "         all_label_model2[:-1000])\n",
    "svc2.fit(np.concatenate((all_densefeatures_model2[:-1000], all_sparsefeatures_model2[:-1000]), axis = -1), \\\n",
    "         all_label_model2[:-1000])\n",
    "\n",
    "stacking_model_2 = StackingClassifier(\n",
    "    estimators = [('xgb', xgb2), ('mlp', mlp2), ('svc', svc2)],\n",
    "    final_estimator = LogisticRegression())\n",
    "\n",
    "stacking_model_2.fit(np.concatenate((all_densefeatures_model2[:-1000], all_sparsefeatures_model2[:-1000]), axis = -1), \\\n",
    "                     all_label_model2[:-1000])\n",
    "\n",
    "### FIXME: predict\n",
    "y_pred_2 = stacking_model_2.predict(np.concatenate((all_densefeatures_model2[-1000:], all_sparsefeatures_model2[-1000:]), axis = -1))\n",
    "y_eval_2 = all_label_model2[-1000:]\n",
    "\n",
    "\n",
    "\n",
    "acc_count_2 = 0\n",
    "for _ in range(y_pred_2.shape[0]):\n",
    "    if y_pred_2[_] == y_eval_2[_]: acc_count_2 += 1\n",
    "\n",
    "print ('model2 accurate rate: ' + str(acc_count_2 / y_eval_2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391c805",
   "metadata": {},
   "source": [
    "### Ablation Study: Without Feature Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23484d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pdb.set_trace()\n",
    "\n",
    "print ('\\n')\n",
    "print ('=================train ensemble model3=====================')\n",
    "xgb1 = XGBClassifier(learning_rate = 0.01, n_estimators = 20, max_depth = 8, \\\n",
    "                     min_child_weight = 1, gamma = 0., subsample = 1, \\\n",
    "                     colsample_btree = 1, scale_pos_weight = 1, random_state = 100, slient = 0)\n",
    "mlp1 = MLPClassifier()\n",
    "svc1 = SVC()\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "all_train_features = np.concatenate((all_features_model1[:-1000], all_features_model2[:-1000]), axis = 0)\n",
    "all_train_labels = np.concatenate((all_label_model1[:-1000], all_label_model2[:-1000]), axis = 0)\n",
    "\n",
    "xgb1.fit(all_train_features, all_train_labels)\n",
    "mlp1.fit(all_train_features, all_train_labels)\n",
    "svc1.fit(all_train_features, all_train_labels)\n",
    "\n",
    "stacking_model_3 = StackingClassifier(\n",
    "    estimators = [('xgb', xgb1), ('mlp', mlp1), ('svc', svc1)],\n",
    "    final_estimator = LogisticRegression())\n",
    "\n",
    "stacking_model_3.fit(all_train_features, all_train_labels)\n",
    "\n",
    "def cal_overall_accuracy(all_input_features, all_ground_truth_label_s):\n",
    "    all_y_pred = stacking_model_3.predict(all_input_features)\n",
    "    \n",
    "    # acc = 0\n",
    "    # pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    assert all_y_pred.shape[0] == all_ground_truth_label_s.shape[0]\n",
    "    \n",
    "    return np.sum(all_y_pred == all_ground_truth_label_s) / all_y_pred.shape[0]\n",
    "    \n",
    "print ('Model results without feature split: ')    \n",
    "print (cal_overall_accuracy(all_features_model1[-1000:], all_label_model1[-1000:]))\n",
    "print (cal_overall_accuracy(all_features_model2[-1000:], all_label_model2[-1000:]))\n",
    "\n",
    "############# AUTOML: (autogluon ?) ####################\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710f476",
   "metadata": {},
   "source": [
    "### Next Ablation Study: Fourier Transform (Fourier Analysis) & Floor divide & etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69e789",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### test #######################\n",
    "\n",
    "# pdb.set_trace()\n",
    "ret = []\n",
    "for _ in range(test_data.shape[0]):\n",
    "    if test_data['CryoSleep'][_] == True:\n",
    "        tmp_ret = xgb1.predict(np.concatenate((test_sparse_features[_, 0].reshape(-1, 1), \\\n",
    "                               test_sparse_features[_, 2:].reshape(1, -1), \\\n",
    "                               test_dense_features[_, :].reshape(1, -1)), axis = -1))\n",
    "\n",
    "    else:\n",
    "        tmp_ret = xgb2.predict(np.concatenate((test_sparse_features[_, 0].reshape(1, -1), \\\n",
    "                               test_sparse_features[_, 2:].reshape(1, -1), \\\n",
    "                               test_dense_features[_, :].reshape(1, -1)), axis = -1))\n",
    "    if tmp_ret == 1: ret.append(True)\n",
    "    else: ret.append(False)\n",
    "\n",
    "\n",
    "ret_df = {'PassengerId': test_data['PassengerId'], 'Transported': ret}\n",
    "ret_pd_df = pd.DataFrame(ret_df)\n",
    "ret_pd_df.to_csv('./result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
